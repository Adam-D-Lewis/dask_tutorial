{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Introduction to Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This notebook is aimed at new users of Dask.  A working knowledge of pandas is assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "# What is Dask?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask allows users to work with datasets too large to fit in memory while using a similar syntax to some of the most popular python packages.  Dask has high level API's similar to the following packages:\n",
    "- Pandas:  Tabular Categorical and Numeric Data\n",
    "- Numpy:  Numeric Data\n",
    "- Python lists (similar to Spark RDDs):  Unstructured Data\n",
    "\n",
    "Dask also scales well.  It performs well on a single machine and on a cluster with thousands of cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Level API's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Dataframe mimics Pandas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Pandas Syntax                           # Dask DataFrame Syntax\n",
    "import pandas as pd                       import dask.dataframe as dd\n",
    "df = pd.read_csv('2015-01-01.csv')        df = dd.read_csv('2015-*-*.csv')\n",
    "df.groupby(df.user_id).value.mean()       df.groupby(df.user_id).value.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Array mimics Numpy Arrays"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Numpy Syntax                           # Dask Array Syntax\n",
    "import numpy as np                       import dask.array as da\n",
    "f = h5py.File('myfile.hdf5')             f = h5py.File('myfile.hdf5')\n",
    "x = np.array(f['/small-data'])           x = da.from_array(f['/big-data'], chunks=(1000, 1000))\n",
    "x - x.mean(axis=1)                       x - x.mean(axis=1).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Bag mimics iterators, Toolz, and PySpark"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import dask.bag as db\n",
    "b = db.read_text('2015-*-*.json.gz').map(json.loads)\n",
    "b.pluck('name').frequencies().topk(10, lambda pair: pair[1]).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask DataFrame Under the Hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dask divides a a large dataset that potentially can't fit into memory into many smaller pandas dataframes.  Each pandas dataframe makes up a partition.\n",
    "- Dask will load in memory and work with each of the partitions as needed to perform the requested operations.\n",
    "- The user can and sometimes needs to specify the size of dask partitions.  Dask partitions should fit comfortably in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` Rule of Thumb: Keep Partition sizes ~ 100MB.`\n",
    "\n",
    "`Better forumula: total_memory/(9*cores)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the high and low level API's (Collections) build task graphs consisting of the requested operations that are then run in an appropriate order by a Scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='dask-overview.jpg' style=\"width:70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Various Scheduler Options Exist\n",
    "    - Local Threads - Default for Dask Array, Dask Dataframe, Dask Delayed\n",
    "    - Local Processes - Recommended to use Distributed Instead\n",
    "    - Synchronous (Debugging)\n",
    "    - Distributed - Works on a cluster or run locally on a single machine\n",
    "        - Dashboard\n",
    "        - Better Data Locality\n",
    "\n",
    "https://docs.dask.org/en/latest/scheduling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask, dask.dataframe as dd\n",
    "import distributed\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:40795</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>8.26 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:40795' processes=2 threads=2, memory=8.26 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = distributed.LocalCluster(n_workers=2, threads_per_worker=1) \n",
    "client = distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_GB = dask.datasets.timeseries(start='2000-01-01',\n",
    "                        end='2000-01-31',\n",
    "                        freq='1s',\n",
    "                        partition_freq='1d',\n",
    "                        seed=42).compute().memory_usage(deep=True).sum()/2**30\n",
    "print(f'~{size_GB:0.2f} GB for 1 month')\n",
    "print(f'~{size_GB*12*5:0.2f} GB for 5 years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -h |  tail -n +2 | awk '{ print $1 $3 \" used\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dask.datasets.timeseries(start='2000-01-01',\n",
    "                        end='2000-06-30',\n",
    "                        freq='1s',\n",
    "                        partition_freq='1d',\n",
    "                        seed=42)\n",
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -h |  tail -n +2 | awk '{ print $1 $3 \" used\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Couldn't work with this in pandas on my machine, but I can in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheap/Expensive Operations in Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheap Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trivially parallelizable operations (fast):\n",
    "\n",
    "- Element-wise operations: df.x + df.y, df * df\n",
    "- Row-wise selections: df[df.x > 0]\n",
    "- Indexing by Value: df.loc[4.0:10.5]\n",
    "- Common aggregations: df.x.max(), df.max()\n",
    "- Is in: df[df.x.isin([1, 2, 3])]\n",
    "- Date time/string accessors: df.timestamp.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleverly parallelizable operations (fast):\n",
    "\n",
    "- groupby-aggregate (with common aggregations): df.groupby(df.x).y.max(), df.groupby('x').max()\n",
    "- groupby-apply on index: df.groupby(['idx', 'x']).apply(myfunc), where idx is the index level name\n",
    "- value_counts: df.x.value_counts()\n",
    "- Drop duplicates: df.x.drop_duplicates()\n",
    "- Join on index: dd.merge(df1, df2, left_index=True, right_index=True) or dd.merge(df1, df2, on=['idx', 'x']) where idx is the - index name for both df1 and df2\n",
    "- Join with Pandas DataFrames: dd.merge(df1, df2, on='id')\n",
    "- Element-wise operations with different partitions / divisions: df1.x + df2.y\n",
    "- Date time resampling: df.resample(...)\n",
    "- Rolling averages: df.rolling(...)\n",
    "- Pearson’s correlation: df[['col1', 'col2']].corr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expensive Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations requiring a shuffle (slow-ish, unless on index)\n",
    "- Set index: df.set_index(df.x)\n",
    "- groupby-apply not on index (with anything): df.groupby(df.x).apply(myfunc)\n",
    "- Join not on the index: dd.merge(df1, df2, on='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Number of Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, fewer partitions is better, but each partition needs needs to comfortably fit in memory.  Dask commonly has 2-3 partitions in memory at a time so it always has something to work on.  \n",
    "\n",
    "The reason fewer partitions is better is each task has an overhead of ~1 ms, and more paritions means more tasks and thus more overhead.  The overhead from a few thousand tasks is usually not going to be noticeable.\n",
    "\n",
    "- https://docs.dask.org/en/latest/best-practices.html#avoid-very-large-partitions\n",
    "- https://docs.dask.org/en/latest/best-practices.html#avoid-very-large-graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trivial Example of Too Many Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://127.0.0.1:36410 remote=tcp://127.0.0.1:40795>\n",
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://127.0.0.1:36414 remote=tcp://127.0.0.1:40795>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.4 s, sys: 2.33 s, total: 46.7 s\n",
      "Wall time: 57.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5001634.491113036"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import dask.array as da\n",
    "CHUNK_SIZE = 1000 # bytes\n",
    "x = da.random.random(10_000_000, chunks=CHUNK_SIZE)\n",
    "x.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.86 s, sys: 186 ms, total: 4.04 s\n",
      "Wall time: 4.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4999800.495494896"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import dask.array as da\n",
    "CHUNK_SIZE = 10_000 # bytes\n",
    "x = da.random.random(10_000_000, chunks=CHUNK_SIZE)\n",
    "x.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You really should be doing this computation in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the statistical profiler (requires Bokeh?) in the link above, It's a good way to debug.\n",
    "\n",
    "- Task stream \n",
    "    - Each line is a thread\n",
    "    - Color Coordination with tasks below in Progress Bar\n",
    "    - Red rectangle is worker communication\n",
    "- Progress Bars \n",
    "    - Gray means ready to be computed, but waiting\n",
    "    - Bright Color means in memory\n",
    "    - Transparentish colored portion to the left means processed, and no longer in memory\n",
    "- Tasks Processing\n",
    "    - Shows how many tasks are ready to be processed by the worker, red means not enough tasks queued to keep worker busy\n",
    "- Bytes Stored\n",
    "    - Shows how much memory each worker has in memory at any given time.  Turns orange if it get's to a certain level, and red if it's higher still.\n",
    "- Many other features, click on other tabs to see\n",
    "\n",
    "- https://distributed.dask.org/en/latest/diagnosing-performance.html\n",
    "- https://www.youtube.com/watch?v=N_GqzcuGLCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you might open a large data set, reduce the size by grouping somehow.  It's a good idea to repartition after doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dask.datasets.timeseries(start='2000-01-01',\n",
    "                        end='2001-06-30',\n",
    "                        freq='1s',\n",
    "                        partition_freq='1d',\n",
    "                        seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby((df.index.year-df[0].year)*1000+df.index.dayofyear).sum()\n",
    "df = df.repartition(npartitions=df.npartions//60**2)\n",
    "df = df.persist() # to save calculations for many further calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000001</td>\n",
       "      <td>86398019</td>\n",
       "      <td>-6.885473</td>\n",
       "      <td>41.831835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000002</td>\n",
       "      <td>86403247</td>\n",
       "      <td>52.697918</td>\n",
       "      <td>-75.171130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000003</td>\n",
       "      <td>86396708</td>\n",
       "      <td>-388.171180</td>\n",
       "      <td>76.150513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000004</td>\n",
       "      <td>86399204</td>\n",
       "      <td>-172.614676</td>\n",
       "      <td>-20.051186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000005</td>\n",
       "      <td>86392161</td>\n",
       "      <td>256.048209</td>\n",
       "      <td>48.594984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id           x          y\n",
       "timestamp                                 \n",
       "2000001    86398019   -6.885473  41.831835\n",
       "2000002    86403247   52.697918 -75.171130\n",
       "2000003    86396708 -388.171180  76.150513\n",
       "2000004    86399204 -172.614676 -20.051186\n",
       "2000005    86392161  256.048209  48.594984"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When NOT to use Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is a great tool when the data is too large to work with in memory, but it is not the best tool to use for all use cases.\n",
    "\n",
    "In the Dask documentation Best Practices Section (https://docs.dask.org/en/latest/best-practices.html):\n",
    "- Dask documentation recommends using plain pandas, numpy, etc. when those tools are appropriate over dask.\n",
    "- Dask documentation also recommends a common workflow of reducing a large dataset (e.g. with statistics) with Dask until it can fit in memory, and then switching to pandas, numpy, etc.\n",
    "- Parallelism brings extra complexity and overhead. Sometimes it’s necessary for larger problems, but often it’s not. Before adding a parallel computing system like Dask to your workload you may want to first try some alternatives:\n",
    "    - Better Algorithms\n",
    "    - Better File Formats\n",
    "    - Compiled Code\n",
    "    - Sampling\n",
    "    - Reduce Bottlenecks via Profiling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Level API's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Delayed mimics for loops and wraps custom code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "L = []\n",
    "for fn in filenames:                  # Use for loops to build up computation\n",
    "    data = delayed(load)(fn)          # Delay execution of function\n",
    "    L.append(delayed(process)(data))  # Build connections between variables\n",
    "\n",
    "result = delayed(summarize)(L)\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "lazy_results = [ ]\n",
    "for a in A:\n",
    "    for b in B:\n",
    "        if a < b:\n",
    "            c = dask.delayed(f)(a, b) # add lazy task\n",
    "        else: \n",
    "            c = dask.delayed(g)(a, b) # add lazy task\n",
    "        lazy_results.append(c)\n",
    "\n",
    "\n",
    "results = dask.compute(*lazy_results) # compute all in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be purposeful in choosing threads vs processes for dask.  If using lots of pandas/numpy functions then threads are better, if using lots of native python functions, processes are better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Futures interface provides general submission of custom tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client('scheduler:port')\n",
    "\n",
    "futures = []\n",
    "for fn in filenames:\n",
    "    future = client.submit(load, fn)\n",
    "    futures.append(future)\n",
    "\n",
    "summary = client.submit(summarize, futures)\n",
    "summary.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "https://anaconda.org/jonmmease/spatial-partitioning-for-datashader-points-rendering/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Hyperlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask Official Documentation - https://docs.dask.org/en/latest/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://docs.dask.org/en/latest/\" width=\"900\" height=\"400\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://docs.dask.org/en/latest/\" width=\"900\" height=\"400\"></iframe>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
