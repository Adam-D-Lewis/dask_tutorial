{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225e7f18-29fb-47fb-bd1d-fd63c644ba67",
   "metadata": {},
   "source": [
    "# This notebook examines the read/write and partitioning structure of pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d0f2d0-90fb-4cf0-8f2f-21cf0c9d606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "import dask\n",
    "from distributed import Client\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c01e3c-0f34-4fff-b393-82c01a1046bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/balast/CodingProjects/dask_tutorials/parquet-deep-dive/dask-worker-space/worker-r5spyf4k', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/balast/CodingProjects/dask_tutorials/parquet-deep-dive/dask-worker-space/worker-cagx6nlk', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/balast/CodingProjects/dask_tutorials/parquet-deep-dive/dask-worker-space/worker-2jt1dykl', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/balast/CodingProjects/dask_tutorials/parquet-deep-dive/dask-worker-space/worker-9g77irot', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/balast/CodingProjects/dask_tutorials/parquet-deep-dive/dask-worker-space/worker-akoe7ckp', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/balast/CodingProjects/dask_tutorials/parquet-deep-dive/dask-worker-space/worker-al1muolv', purging\n"
     ]
    }
   ],
   "source": [
    "c = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef651be-a572-4abe-bff3-f7543db24ed7",
   "metadata": {},
   "source": [
    "# Generate Trade Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59a81ae-0e4d-4aff-b451-5bd6639863ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trade_files_per_ns = 200_000\n",
    "n_paths = 50_000\n",
    "n_dates = 120\n",
    "max_paths_per_partition = 50_000 # 10 # 200\n",
    "\n",
    "memory_needed_per_row_gb = 138/1024/6_000_000\n",
    "n_partitions = math.ceil(n_paths/max_paths_per_partition)\n",
    "min_node_memory_needed_gb = (memory_needed_per_row_gb * n_paths * n_dates / n_partitions) * max_trade_files_per_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad75e99d-1272-4567-a541-d5a57466dfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26953.125"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_node_memory_needed_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c29bd8e-7661-4e27-9ac2-9214838411e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "n_files_to_create = 1\n",
    "trade_folder = './datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab95182-8270-4401-84a7-6634020c95d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating File 000\n"
     ]
    }
   ],
   "source": [
    "dates = pd.date_range(\"2021-07-01\", freq=\"3M\", periods=n_dates)\n",
    "for file_no in range(n_files_to_create):\n",
    "    print(f'Creating File {file_no:03d}')\n",
    "    for path in range(n_paths):\n",
    "        for date in dates:\n",
    "            all_rows.append([date, path, random.random()*200-100]) \n",
    "    \n",
    "    df = pd.DataFrame(all_rows, columns=['date', 'path', 'mtm']).astype({'path': 'int32', 'mtm': 'float64'}).set_index('path')\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    ddf = dd.from_pandas(df, npartitions=n_partitions)\n",
    "    trade_filepath = f'{trade_folder}/row_partitioned_trade_file_{file_no:03d}.parquet'\n",
    "    ddf.to_parquet(trade_filepath, row_group_size=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8289d9fb-1b45-4c77-9980-5f2cb27135ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('datasets/row_partitioned_trade_file_000.parquet')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = sorted(list(Path(trade_folder).glob('*.parquet')))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74456a3c-a2df-45e3-9b52-7eea00a01dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mpq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_pandas_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mread_dictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilesystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_legacy_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_prefixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpre_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcoerce_int96_timestamp_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Read a Table from Parquet format\n",
       "\n",
       "Note: starting with pyarrow 1.0, the default for `use_legacy_dataset` is\n",
       "switched to False.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "source: str, pyarrow.NativeFile, or file-like object\n",
       "    If a string passed, can be a single file name or directory name. For\n",
       "    file-like objects, only read a single file. Use pyarrow.BufferReader to\n",
       "    read a file contained in a bytes or buffer-like object.\n",
       "columns: list\n",
       "    If not None, only these columns will be read from the file. A column\n",
       "    name may be a prefix of a nested field, e.g. 'a' will select 'a.b',\n",
       "    'a.c', and 'a.d.e'.\n",
       "use_threads : bool, default True\n",
       "    Perform multi-threaded column reads.\n",
       "metadata : FileMetaData\n",
       "    If separately computed\n",
       "read_dictionary : list, default None\n",
       "    List of names or column paths (for nested types) to read directly\n",
       "    as DictionaryArray. Only supported for BYTE_ARRAY storage. To read\n",
       "    a flat column as dictionary-encoded pass the column name. For\n",
       "    nested types, you must pass the full column \"path\", which could be\n",
       "    something like level1.level2.list.item. Refer to the Parquet\n",
       "    file's schema to obtain the paths.\n",
       "memory_map : bool, default False\n",
       "    If the source is a file path, use a memory map to read file, which can\n",
       "    improve performance in some environments.\n",
       "buffer_size : int, default 0\n",
       "    If positive, perform read buffering when deserializing individual\n",
       "    column chunks. Otherwise IO calls are unbuffered.\n",
       "partitioning : Partitioning or str or list of str, default \"hive\"\n",
       "    The partitioning scheme for a partitioned dataset. The default of \"hive\"\n",
       "    assumes directory names with key=value pairs like \"/year=2009/month=11\".\n",
       "    In addition, a scheme like \"/2009/11\" is also supported, in which case\n",
       "    you need to specify the field names or a full schema. See the\n",
       "    ``pyarrow.dataset.partitioning()`` function for more details.\n",
       "use_pandas_metadata : bool, default False\n",
       "    If True and file has custom pandas schema metadata, ensure that\n",
       "    index columns are also loaded.\n",
       "use_legacy_dataset : bool, default False\n",
       "    By default, `read_table` uses the new Arrow Datasets API since\n",
       "    pyarrow 1.0.0. Among other things, this allows to pass `filters`\n",
       "    for all columns and not only the partition keys, enables\n",
       "    different partitioning schemes, etc.\n",
       "    Set to True to use the legacy behaviour.\n",
       "ignore_prefixes : list, optional\n",
       "    Files matching any of these prefixes will be ignored by the\n",
       "    discovery process if use_legacy_dataset=False.\n",
       "    This is matched to the basename of a path.\n",
       "    By default this is ['.', '_'].\n",
       "    Note that discovery happens only if a directory is passed as source.\n",
       "filesystem : FileSystem, default None\n",
       "    If nothing passed, paths assumed to be found in the local on-disk\n",
       "    filesystem.\n",
       "filters : List[Tuple] or List[List[Tuple]] or None (default)\n",
       "    Rows which do not match the filter predicate will be removed from scanned\n",
       "    data. Partition keys embedded in a nested directory structure will be\n",
       "    exploited to avoid loading files at all if they contain no matching rows.\n",
       "    If `use_legacy_dataset` is True, filters can only reference partition\n",
       "    keys and only a hive-style directory structure is supported. When\n",
       "    setting `use_legacy_dataset` to False, also within-file level filtering\n",
       "    and different partitioning schemes are supported.\n",
       "\n",
       "    Predicates are expressed in disjunctive normal form (DNF), like\n",
       "    ``[[('x', '=', 0), ...], ...]``. DNF allows arbitrary boolean logical\n",
       "    combinations of single column predicates. The innermost tuples each\n",
       "    describe a single column predicate. The list of inner predicates is\n",
       "    interpreted as a conjunction (AND), forming a more selective and\n",
       "    multiple column predicate. Finally, the most outer list combines these\n",
       "    filters as a disjunction (OR).\n",
       "\n",
       "    Predicates may also be passed as List[Tuple]. This form is interpreted\n",
       "    as a single conjunction. To express OR in predicates, one must\n",
       "    use the (preferred) List[List[Tuple]] notation.\n",
       "\n",
       "    Each tuple has format: (``key``, ``op``, ``value``) and compares the\n",
       "    ``key`` with the ``value``.\n",
       "    The supported ``op`` are:  ``=`` or ``==``, ``!=``, ``<``, ``>``, ``<=``,\n",
       "    ``>=``, ``in`` and ``not in``. If the ``op`` is ``in`` or ``not in``, the\n",
       "    ``value`` must be a collection such as a ``list``, a ``set`` or a\n",
       "    ``tuple``.\n",
       "\n",
       "    Examples:\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        ('x', '=', 0)\n",
       "        ('y', 'in', ['a', 'b', 'c'])\n",
       "        ('z', 'not in', {'a','b'})\n",
       "\n",
       "    \n",
       "pre_buffer : bool, default True\n",
       "    Coalesce and issue file reads in parallel to improve performance on\n",
       "    high-latency filesystems (e.g. S3). If True, Arrow will use a\n",
       "    background I/O thread pool. This option is only supported for\n",
       "    use_legacy_dataset=False. If using a filesystem layer that itself\n",
       "    performs readahead (e.g. fsspec's S3FS), disable readahead for best\n",
       "    results.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "pyarrow.Table\n",
       "    Content of the file as a table (of columns)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.conda/envs/dask/lib/python3.9/site-packages/pyarrow/parquet.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pq.read_table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b9935de-8edf-4f09-8c6d-990f26d5905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 ms ± 8.21 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "pd.read_parquet(files[0])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "831c0a10-9a1b-43c4-b1f5-420cdb123ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 ms ± 3.54 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "pd.read_parquet(files[0], columns=['mtm'])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1664f08-8946-4f4a-9a8c-f21f5caf261a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Expected file path, but datasets/row_partitioned_trade_file_000.parquet is a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23547/2083016278.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParquetFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/dask/lib/python3.9/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, metadata, common_metadata, read_dictionary, memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit)\u001b[0m\n\u001b[1;32m    226\u001b[0m                  pre_buffer=False, coerce_int96_timestamp_unit=None):\n\u001b[1;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParquetReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         self.reader.open(\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_memory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dask/lib/python3.9/site-packages/pyarrow/_parquet.pyx\u001b[0m in \u001b[0;36mpyarrow._parquet.ParquetReader.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dask/lib/python3.9/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.get_reader\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dask/lib/python3.9/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.get_native_file\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dask/lib/python3.9/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.OSFile.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dask/lib/python3.9/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._check_is_file\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Expected file path, but datasets/row_partitioned_trade_file_000.parquet is a directory"
     ]
    }
   ],
   "source": [
    "table = pq.ParquetFile(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2afc8b6f-ae83-495a-91cc-c47330ab651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pq.ParquetFile(files[0] / 'part.0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75311e7e-c1cb-4ac0-ac07-0e731fa53926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x7f4e300eaae0>\n",
       "  created_by: parquet-cpp-arrow version 5.0.0\n",
       "  num_columns: 3\n",
       "  num_rows: 6000000\n",
       "  num_row_groups: 60\n",
       "  format_version: 1.0\n",
       "  serialized_size: 22364"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7a049-cd0c-4c3f-97e9-65b36b89d08a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
